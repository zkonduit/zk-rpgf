{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "from core import constants\n",
                "from core.utils import *\n",
                "\n",
                "log = get_logger()\n",
                "\n",
                "pd.set_option(\"display.float_format\", \"{:.2f}\".format)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(\"data/anonymized_project_votes.csv\")\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "official_df = pd.read_csv(\"data/rpgf3_results.csv\")\n",
                "official_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Calculate Voting Results using pytorch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "allocator = ProjectAllocator(\n",
                "    total_amount=constants.TOTAL_AMOUNT,\n",
                "    min_amount=constants.MIN_AMOUNT,\n",
                "    quorum=constants.QUORUM,\n",
                ")\n",
                "\n",
                "\n",
                "project_tensors = allocator.convert_anonymized_df_to_tensors(df)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "# export to onnx\n",
                "allocator.eval()\n",
                "project_tensors_trunc = project_tensors\n",
                "\n",
                "\n",
                "\n",
                "input_names = ['input_' + str(i) for i in range(len(project_tensors_trunc))]\n",
                "    # Export the model\n",
                "torch.onnx.export(allocator,               # model being run\n",
                "                      tuple(project_tensors_trunc),          # model input (or a tuple for multiple inputs)\n",
                "                      \"network.onnx\",           # where to save the model (can be a file or file-like object)\n",
                "                      export_params=False,       # store the trained parameter weights inside the model file\n",
                "                      opset_version=17,         # the ONNX version to export the model to\n",
                "                      do_constant_folding=False, # whether to execute constant folding for optimization\n",
                "                      input_names = input_names,   # the model's input names\n",
                "                      output_names = ['output'])\n",
                "\n",
                "data_path = os.path.join(\"input.json\")\n",
                "data = dict(input_data = [tensor.detach().numpy().reshape([-1]).tolist() for tensor in project_tensors_trunc])\n",
                "# Serialize data into file:\n",
                "json.dump(data, open(data_path, 'w'))\n",
                "\n",
                "# panic and exit\n",
                "exit(1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "# convert projects tensors to tuple\n",
                "final_allocation_torch = allocator.forward(*project_tensors_trunc, num_iterations=1)\n",
                "                                           \n",
                "# a bunch of code to compare pytorch output and the pandas output as a sanity check\n",
                "final_allocation_torch_np = final_allocation_torch.detach().numpy()\n",
                "final_allocation_torch_df = pd.DataFrame(final_allocation_torch_np, columns=[\"scaled_amount\"])\n",
                "# add index project_id\n",
                "final_allocation_torch_df = final_allocation_torch_df.set_index(df['project_name'])\n",
                "\n",
                "# sorted vy project id\n",
                "final_allocation_torch_df = final_allocation_torch_df.sort_index()\n",
                "# sort final allocation by project id\n",
                "final_allocation_sorted = official_df\n",
                "# renamed OP Received to match the other dataframe\n",
                "final_allocation_sorted = final_allocation_sorted.rename(columns={\"OP Received\": \"scaled_amount\"})\n",
                "# turn the Project Name into the index\n",
                "final_allocation_sorted = final_allocation_sorted.set_index(\"Project Name\")\n",
                "# now sort it \n",
                "final_allocation_sorted = final_allocation_sorted.sort_index()\n",
                "\n",
                "\n",
                "display(final_allocation_torch_df.head(10))\n",
                "display(final_allocation_sorted.head(10))\n",
                "# compare the two outputs\n",
                "print(\"Are the two outputs equal?\")\n",
                "print(final_allocation_torch_df.compare(final_allocation_sorted[[\"scaled_amount\"]]))\n",
                "\n",
                "# print mean percent diff \n",
                "percent_diff = np.abs(final_allocation_torch_df[\"scaled_amount\"] - final_allocation_sorted[\"scaled_amount\"]) / final_allocation_sorted[\"scaled_amount\"]\n",
                "print(f\"Mean percent diff: {percent_diff.mean() * 100:.2f}%\")\n",
                "# print max percent diff\n",
                "print(f\"Max percent diff: {percent_diff.max() * 100:.2f}%\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now include sanity checks for the data. We will check the following:\n",
                "- that the pandas and pytorch dataframes are the same\n",
                "- that the median and scaled median allocations are the same\n",
                "- that eligibility is the same"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "TODO: investigate the small differences in the scaled median allocations. Seems like pandas adds some small epsilons during calculations that compound."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "After which we can proceed to generate the settings file for `ezkl` and run calibrate settings to find the optimal settings for `ezkl`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ezkl\n",
                "import os\n",
                "\n",
                "model_path = \"network.onnx\"\n",
                "settings_path = \"settings.json\"\n",
                "compiled_model_path = os.path.join('network.compiled')\n",
                "pk_path = os.path.join('test.pk')\n",
                "vk_path = os.path.join('test.vk')\n",
                "settings_path = os.path.join('settings.json')\n",
                "\n",
                "witness_path = os.path.join('witness.json')\n",
                "data_path = os.path.join('input.json')\n",
                "\n",
                "py_run_args = ezkl.PyRunArgs()\n",
                "py_run_args.input_visibility = \"hashed\"\n",
                "py_run_args.output_visibility = \"public\"\n",
                "py_run_args.param_visibility = \"fixed\" # private by default\n",
                "py_run_args.num_inner_cols = 2\n",
                "# the inputs are integer values !\n",
                "py_run_args.input_scale = 0\n",
                "# the params should be very large\n",
                "py_run_args.param_scale = 32\n",
                "py_run_args.scale_rebase_multiplier = 1\n",
                "py_run_args.rebase_frac_zero_constants = True\n",
                "\n",
                "res = ezkl.gen_settings(model_path, settings_path, py_run_args=py_run_args)\n",
                "assert res == True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "ezkl.calibrate_settings(data_path, model_path, settings_path, \"accuracy\", lookup_safety_margin=2, scales=[30], only_range_check_rebase=True, scale_rebase_multiplier=[1])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we will compile the model. The compilation step allow us to generate proofs faster."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
                "assert res == True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Mock prove as a sanity check to ensure that the model is working as expected.\n",
                "\n",
                "Finally, we will generate the proofs and submit the results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# now generate the witness file \n",
                "\n",
                "res = ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
                "assert os.path.isfile(witness_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "res = ezkl.mock(witness_path, compiled_model_path)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Before we can setup the circuit params, we need a SRS (Structured Reference String). The SRS is used to generate the proofs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# srs path\n",
                "res = ezkl.get_srs(settings_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now run setup, this will generate a proving key (pk) and verification key (vk). The proving key is used for proving while the verification key is used for verificaton."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# setup\n",
                "res = ezkl.setup(\n",
                "        compiled_model_path,\n",
                "        vk_path,\n",
                "        pk_path,\n",
                "    )\n",
                "\n",
                "assert res == True\n",
                "assert os.path.isfile(vk_path)\n",
                "assert os.path.isfile(pk_path)\n",
                "assert os.path.isfile(settings_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GENERATE A PROOF\n",
                "proof_path = os.path.join('test.pf')\n",
                "\n",
                "res = ezkl.prove(\n",
                "        witness_path,\n",
                "        compiled_model_path,\n",
                "        pk_path,\n",
                "        proof_path,\n",
                "        \"single\",\n",
                "    )\n",
                "\n",
                "print(res)\n",
                "assert os.path.isfile(proof_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now verify the proof. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# VERIFY IT\n",
                "res = ezkl.verify(\n",
                "        proof_path,\n",
                "        settings_path,\n",
                "        vk_path,\n",
                "    )\n",
                "\n",
                "assert res == True\n",
                "print(\"verified\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now create an EVM / `.sol` verifier that can be deployed on chain to verify submitted proofs using a view function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "abi_path = 'test.abi'\n",
                "sol_code_path = 'test_1.sol'\n",
                "\n",
                "res = ezkl.create_evm_verifier(\n",
                "        vk_path,\n",
                "        settings_path,\n",
                "        sol_code_path,\n",
                "        abi_path,\n",
                "    )\n",
                "assert res == True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "addr_path_verifier = \"addr_verifier.txt\"\n",
                "\n",
                "res = ezkl.deploy_evm(\n",
                "    addr_path_verifier,\n",
                "    sol_code_path,\n",
                "    'http://127.0.0.1:3030'\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sanity checks on circuit outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import onnx\n",
                "import onnxruntime\n",
                "import os\n",
                "import json\n",
                "import ezkl\n",
                "import numpy as np\n",
                "\n",
                "witness_path = os.path.join('witness.json')\n",
                "settings_path = os.path.join('settings.json')\n",
                "model_path = os.path.join('network.onnx')\n",
                "data_path = os.path.join('input.json')\n",
                "\n",
                "def get_ezkl_output(witness_file, settings_file):\n",
                "    # convert the quantized ezkl output to float value\n",
                "    witness_output = json.load(open(witness_file))\n",
                "    outputs = witness_output['outputs']\n",
                "    with open(settings_file) as f:\n",
                "        settings = json.load(f)\n",
                "    ezkl_outputs = [[ezkl.string_to_float(\n",
                "        outputs[i][j], settings['model_output_scales'][i]) for j in range(len(outputs[i]))] for i in range(len(outputs))]\n",
                "    return ezkl_outputs\n",
                "\n",
                "\n",
                "def get_onnx_output(model_file, input_file):\n",
                "    # generate the ML model output from the ONNX file\n",
                "    onnx_model = onnx.load(model_file)\n",
                "    onnx.checker.check_model(onnx_model)\n",
                "\n",
                "    with open(input_file) as f:\n",
                "        inputs = json.load(f)\n",
                "    # reshape the input to the model\n",
                "    num_inputs = len(inputs['input_data'])\n",
                "\n",
                "    onnx_input = dict()\n",
                "    for i in range(num_inputs):\n",
                "        input_node = onnx_model.graph.input[i]\n",
                "        dims = []\n",
                "        elem_type = input_node.type.tensor_type.elem_type\n",
                "        for dim in input_node.type.tensor_type.shape.dim:\n",
                "            if dim.dim_value == 0:\n",
                "                dims.append(1)\n",
                "            else:\n",
                "                dims.append(dim.dim_value)\n",
                "        if elem_type == 7:\n",
                "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
                "                np.int64).reshape(dims)\n",
                "        elif elem_type == 9:\n",
                "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
                "                bool).reshape(dims)\n",
                "        else:\n",
                "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
                "                np.float32).reshape(dims)\n",
                "        onnx_input[input_node.name] = inputs_onnx\n",
                "    try:\n",
                "        onnx_session = onnxruntime.InferenceSession(model_file)\n",
                "        onnx_output = onnx_session.run(None, onnx_input)\n",
                "    except Exception as e:\n",
                "        print(\"Error in ONNX runtime: \", e)\n",
                "        print(\"using inputs[output_data]\")\n",
                "        onnx_output = inputs['output_data']\n",
                "    return onnx_output[0]\n",
                "\n",
                "\n",
                "def compare_outputs(zk_output, onnx_output):\n",
                "    # calculate hamming difference between the 2 outputs (which are lists)\n",
                "\n",
                "    res = []\n",
                "\n",
                "\n",
                "    contains_sublist = any(isinstance(sub, list) for sub in zk_output)\n",
                "    if contains_sublist:\n",
                "        try:\n",
                "            if len(onnx_output) == 1:\n",
                "                zk_output = zk_output[0]\n",
                "        except Exception as e:\n",
                "            zk_output = zk_output[0]\n",
                "\n",
                "    flat_zk_output = np.array(zk_output).flatten()\n",
                "    flat_onnx_output = np.array(onnx_output).flatten()\n",
                "\n",
                "    # select every 4th element\n",
                "    scaled_amount_zk = flat_zk_output[3::4]\n",
                "    scaled_amount_onnx = flat_onnx_output[3::4]\n",
                "\n",
                "    # print the diff where they are not both 0\n",
                "    print(\"scaled amount diff\")\n",
                "    for i in range(len(scaled_amount_zk)):\n",
                "        if scaled_amount_zk[i] != 0.0 and scaled_amount_onnx[i] != 0.0:\n",
                "            print(\"scaled amount zk: \", scaled_amount_zk[i])\n",
                "            print(\"scaled amount onnx: \", scaled_amount_onnx[i])\n",
                "            print(\"diff: \", scaled_amount_zk[i] - scaled_amount_onnx[i])\n",
                "            print(\"percent diff: \", 100*(scaled_amount_zk[i] - scaled_amount_onnx[i]) / scaled_amount_onnx[i])\n",
                "\n",
                "    zip_object = zip(flat_zk_output[3::4], flat_onnx_output[3::4])\n",
                "\n",
                "    for list1_i, list2_i in zip_object:\n",
                "        if list1_i == 0.0 and list2_i == 0.0:\n",
                "            res.append(0)\n",
                "        else:\n",
                "            percent_diff = (list1_i - list2_i) / list2_i\n",
                "            res.append(percent_diff)\n",
                "\n",
                "\n",
                "    return np.abs(res)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import plotly.express as px\n",
                "import pandas as pd\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "    # get the ezkl output\n",
                "ezkl_output = get_ezkl_output(witness_path, settings_path)\n",
                "    # get the onnx output\n",
                "onnx_output = get_onnx_output(model_path, data_path)\n",
                "    # compare the outputs\n",
                "l1_difference = compare_outputs(ezkl_output, onnx_output)\n",
                "\n",
                "df = pd.DataFrame(l1_difference, columns=[\"percent error\"])\n",
                "\n",
                "\n",
                "# Create a histogram\n",
                "fig = px.histogram(df, x=\"percent error\",\n",
                "                  title=\"Distribution of percent error\")\n",
                "fig.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "analytics-internal-nXLXwD2Z",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.15"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
